# üß¨ CoralX Multi-Modal AI Safety + Gemma3N Configuration
# Optimized for Google Colab environments with comprehensive multi-objective evaluation
# Follows categorical architecture principles with fail-fast validation

# üìÅ CENTRALIZED PATH CONFIGURATION
paths:
  local:
    cache_root: "/content/drive/MyDrive/coralx-gemma3n/cache"
    dataset: "/content/drive/MyDrive/coralx-gemma3n/data/fakenews"
    models: "/content/drive/MyDrive/coralx-gemma3n/models"
    results: "/content/drive/MyDrive/coralx-gemma3n/results"
    adapters: "/content/drive/MyDrive/coralx-gemma3n/adapters"
    exports: "/content/drive/MyDrive/coralx-gemma3n/exports"

# üèÉ EXECUTION CONFIGURATION
execution:
  generations: 3              # Quick evolution for Colab
  population_size: 4          # Small population for memory constraints
  output_dir: "/content/drive/MyDrive/coralx-gemma3n/results"
  
# üß¨ EVOLUTION CONFIGURATION
evo:
  # Cellular Automata parameters (optimized for speed)
  ca:
    grid_size: [4, 4]         # Smaller grid for faster CA evolution
    rule_range: [1, 255]      # Full rule space
    steps_range: [3, 10]      # Fewer steps for speed
    initial_density: 0.3      # Standard density
  
  # LoRA parameter candidates (Colab-friendly)
  rank_candidates: [4, 8, 16]              # Lower ranks for memory efficiency
  alpha_candidates: [8, 16, 32]            # Standard alpha values
  dropout_candidates: [0.05, 0.1, 0.2]    # Standard dropout range
  target_modules: ["q_proj", "v_proj", "o_proj"]  # Gemma3N attention modules

# üéØ EXPERIMENT CONFIGURATION  
experiment:
  target: "fakenews_gemma3n"
  name: "fakenews_detection_colab_evolution"
  
  # Multi-modal dataset configuration
  dataset:
    dataset_path: "/content/drive/MyDrive/coralx-gemma3n/data"
    max_samples: 1000         # Limited for Colab memory
    datasets:                 # Multi-objective evaluation datasets
      - "fake_news"           # P1: Text-based fake news detection
      - "deepfake_audio"      # P1: Audio deepfake detection  
      - "deepfake_video"      # P1: Video deepfake detection
      - "jailbreak_prompts"   # P2: Safety evaluation
      - "clean_holdout"       # P3: False positive measurement
    
  # Model configuration
  model:
    model_name: "google/gemma-2b-it"     # Use 2B for Colab compatibility
    max_seq_length: 512       # Shorter sequences for memory efficiency
    
  # Training configuration
  training:
    simulate_training: true   # Enable for evolutionary efficiency - only final candidates get real training
    batch_size: 1             # Small batch for Colab T4
    gradient_accumulation_steps: 4  # Reduced for efficiency
    max_steps: 50             # Quick training
    max_train_samples: 50     # Reduced for efficiency
    learning_rate: 2e-4
    warmup_steps: 5
    
      # Multi-objective evaluation configuration
    evaluation:
      test_samples: 50          # Quick evaluation
      objectives:               # P1-P6 multi-objective framework
        - "task_skill"          # P1: Macro-AUROC across detection tasks
        - "safety"              # P2: Jailbreak resistance
        - "false_positive_cost" # P3: FPR at 90% recall on clean data
        - "memory_efficiency"   # P4: Peak VRAM/RAM usage
        - "cross_modal_fusion"  # P5: Multimodal vs text-only gain
        - "calibration"         # P6: Expected Calibration Error
    
# üèóÔ∏è INFRASTRUCTURE CONFIGURATION
infra:
  executor: "local"           # Run locally in Colab
  modal:
    app_name: "coralx-gemma3n-colab"
    
# üíæ CACHE CONFIGURATION
cache:
  artifacts_dir: "/content/drive/MyDrive/coralx-gemma3n/cache"
  base_checkpoint: "google/gemma-2b-it"
  cache_metadata: true
  cleanup_threshold: 10       # Aggressive cleanup for storage
  auto_sync: true
  modal_mount: "/cache"
  run_id: null               # Will be generated automatically

# üéöÔ∏è THRESHOLD GATES (Multi-objective AI safety optimization)
threshold:
  base_thresholds:
    bugfix: 0.5              # P1: Task skill (detection AUROC)
    style: 0.6               # P3: UX quality (1 - FPR)  
    security: 0.7            # P2: Safety (jailbreak resistance)
    runtime: 0.4             # P4: Memory efficiency
    syntax: 0.5              # P5+P6: Cross-modal + calibration
    
  max_thresholds:
    bugfix: 0.9              # Maximum detection performance
    style: 0.95              # Minimal false positives
    security: 0.95           # High safety standards
    runtime: 0.9             # High memory efficiency
    syntax: 0.9              # Advanced multimodal capabilities
    
  schedule: "sigmoid"        # Progressive strictness

# üìä MULTI-OBJECTIVE EVALUATION CONFIGURATION
evaluation:
  metrics:
    - "task_skill_auroc"      # P1: Macro-AUROC across detection tasks
    - "safety_score"          # P2: Jailbreak resistance percentage
    - "false_positive_rate"   # P3: FPR at 90% recall
    - "memory_usage_gb"       # P4: Peak memory consumption
    - "cross_modal_gain"      # P5: Multimodal vs text-only improvement
    - "calibration_score"     # P6: 1 - Expected Calibration Error
  
  # Benchmarking configuration
  benchmark:
    test_split: 0.2
    validation_split: 0.1
    
# üå± RANDOM SEED
seed: 42

# ‚öôÔ∏è SYSTEM CONFIGURATION
system:
  fail_fast: true            # Crash immediately on errors
  dependencies:
    - "unsloth"
    - "torch"
    - "transformers"
    - "datasets"
    - "pandas"
    - "scikit-learn"
    - "kaggle"

# üéõÔ∏è CHEAP KNOBS CONFIGURATION
cheap_knobs:
  ca_feature_mapping:
    complexity: "temperature"      # CA complexity ‚Üí generation temperature
    intensity: "top_p"            # CA intensity ‚Üí nucleus sampling
    periodicity: "top_k"          # CA periodicity ‚Üí top-k sampling
    convergence: "repetition_penalty"  # CA convergence ‚Üí repetition penalty
    
  parameter_ranges:
    temperature: [0.3, 1.0]
    top_p: [0.7, 0.95]
    top_k: [20, 100] 
    repetition_penalty: [1.0, 1.3]

# üìà MONITORING CONFIGURATION
monitoring:
  log_level: "INFO"
  track_metrics: true
  save_checkpoints: true
  checkpoint_frequency: 1     # Save every generation
  
# üîß COLAB-SPECIFIC OPTIMIZATIONS
colab:
  gpu_memory_fraction: 0.9    # Use most of available GPU memory
  enable_mixed_precision: true
  gradient_checkpointing: true
  dataloader_num_workers: 2   # Limited for Colab
  pin_memory: false          # Disabled for Colab stability 