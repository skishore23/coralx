# ğŸ§ª CORAL-X MINIMAL TEST CONFIGURATION
# Based on coral_x_codellama_config.yaml with minimal test parameters
# Quick test: 2 generations, 4 genomes, 3 problems

# ğŸ“ CENTRALIZED PATH CONFIGURATION - NO HARDCODED PATHS ALLOWED
paths:
  modal:
    # Modal volume paths (when executor=modal)
    # Note: dataset path corrected to match actual volume structure 
    cache_root: "/cache"
    adapters: "/cache/adapters"
    models: "/cache/models"
    dataset: "/cache/quixbugs_dataset"  # Corrected: actual path in volume
    progress: "/cache/evolution_progress.json"
    emergent_behavior: "/cache/emergent_behavior"
    emergent_alerts: "/cache/emergent_behavior/alerts.json"
    realtime_benchmarks: "/cache/realtime_benchmarks"
    coralx_root: "/root/coralx"
  queue_modal:
    # Queue-based Modal paths (when executor=queue_modal) - COMPLETE PATHS
    cache_root: "/cache"
    cache: "/cache"  # Required for compatibility
    adapters: "/cache/adapters"
    models: "/cache/models"
    dataset: "/cache/quixbugs_dataset"
    progress: "/cache/evolution_progress.json"
    emergent_behavior: "/cache/emergent_behavior"
    emergent_alerts: "/cache/emergent_behavior/alerts.json"
    realtime_benchmarks: "/cache/realtime_benchmarks"
    coralx_root: "/root/coralx"
  local:
    # Local paths (when executor=local)
    cache_root: "./cache"
    cache: "./cache"
    models: "./cache/models"
    adapters: "./cache/adapters"
    dataset: "./cache/quixbugs_dataset"
    progress: "./cache/evolution_progress.json"  # Add progress tracking path
    emergent_behavior: "./cache/emergent_behavior"
    emergent_alerts: "./cache/emergent_behavior/alerts.json"
    realtime_benchmarks: "./cache/realtime_benchmarks"
    coralx_root: "."

cache:
  # artifacts_dir required for compatibility - uses queue_modal executor paths  
  artifacts_dir: "/cache/adapters"  # Category theory compliant adapter cache
  base_checkpoint: codellama/CodeLlama-7b-Python-hf
  cleanup_threshold: 100
  metadata: true
  modal_native: true  # When true, prefers Modal volume for all caching
  run_id: "category_theory_test"  # ğŸ§® CATEGORY THEORY TEST RUN

evaluation:
  # ğŸ§¬ EVOLUTION FITNESS EVALUATION (uses CA-derived cheap knobs)
  adaptive_testing:
    capability_thresholds:
      easy_cutoff: 0.3
      medium_cutoff: 0.6
    enable: true
    max_easy_problems: 2   # ğŸ§ª MINIMAL: Only 2 each
    max_hard_problems: 1   # ğŸ§ª MINIMAL
    max_medium_problems: 2 # ğŸ§ª MINIMAL
  fitness_weights:
    bugfix: 0.3      # Same as production
    runtime: 0.1
    security: 0.25
    style: 0.15
    syntax: 0.2      # NEW: Syntax correctness objective

# Enable simple emergent behavior tracking
emergent_tracking:
  enabled: false                         # ğŸ§ª DISABLED for quick test
  alert_threshold: 0.8
  save_frequency: 20

evo:
  alpha_candidates:  # ğŸ”§ EXPANDED: More diversity for 4 genomes
  - 2.0
  - 4.0
  - 8.0
  - 12.0
  - 16.0
  - 24.0
  - 32.0
  - 48.0
  ca:
    grid_size: [8, 8]    # ğŸ”§ LARGER: 8x8 for more CA diversity
    initial_density: 0.35  # ğŸ”§ SLIGHTLY HIGHER: More initial chaos for diversity
    rule_range: [1, 255] # ğŸ”§ FULL RANGE: All CA rules for max diversity
    steps_range: [5, 30] # ğŸ”§ WIDER: 5-30 steps for more temporal diversity
  diversity:
    # Dynamic diversity adjustment based on cache hit rates and performance
    mode: adaptive           # Options: adaptive, fixed, aggressive
    base_strength: 3.0       # ğŸ”§ HIGHER: Maximum diversity for small population
    max_strength: 4.0        # ğŸ”§ HIGHER: Force diversity
    min_strength: 2.0        # ğŸ”§ HIGHER: Even minimum is high for small pop
    cache_threshold: 0.6     # ğŸ”§ LOWER: Trigger diversity sooner
    plateau_threshold: 0.05  # If improvement < 5% over 3 gens, increase diversity
    plateau_window: 2        # ğŸ§ª SMALLER: 2 generations to check for plateau
  dropout_candidates:  # ğŸ”§ EXPANDED: More dropout options
  - 0.01
  - 0.05
  - 0.1
  - 0.15
  - 0.2
  - 0.25
  - 0.3
  rank_candidates:  # ğŸ”§ EXPANDED: More rank options
  - 2
  - 4
  - 6
  - 8
  - 12
  - 16
  - 24
  - 32
  - 48
  - 64
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

adapter_type: "dora" # "lora" or "dora"

execution:
  generations: 2                  # ğŸ§ª MINIMAL: Only 2 generations
  output_dir: /Users/kishore/neat/coralx/coral_cache/results
  population_size: 4              # ğŸ§ª MINIMAL: Only 4 genomes
  selection_mode: "pareto"
  current_generation: 0  # For emergent behavior tracking (updated automatically)
  
  # ğŸ§¬ GENETIC OPERATIONS TRACKING
  genetic_tracking_dir: "results/genetic_tracking"  # Where to save crossover/mutation data
  survival_rate: 0.5                                # ğŸ§ª 50% survive (higher for small pop)
  crossover_rate: 0.5                              # ğŸ§ª 50% crossover (balanced)
  
  # ğŸ”¬ AUTOMATIC HELD-OUT BENCHMARK CONFIGURATION
  run_held_out_benchmark: true  # Enable automatic held-out benchmark after evolution
  held_out_benchmark_config:
    enabled: true
    run_after_evolution: true      # Run automatically when evolution completes
    save_combined_results: true    # Combine evolution + benchmark results
    fail_on_benchmark_error: false # Don't fail evolution if benchmark has issues
    # ğŸ”¥ CRITICAL: Use neutral parameters to avoid interfering with CA cheap knobs
    neutral_parameters:
      temperature: 0.7             # Fixed neutral (NOT CA-derived)
      top_p: 0.9                  # Fixed neutral (NOT CA-derived) 
      top_k: 50                   # Fixed neutral (NOT CA-derived)
      repetition_penalty: 1.1     # Fixed neutral (NOT CA-derived)
      max_tokens: 200             # ğŸ§ª SHORTER: 200 instead of 300

experiment:
  dataset:
    # path now comes from paths.{modal|local}.dataset - corrected to match volume
    path: "/cache/quixbugs_dataset"  # Corrected: matches actual volume structure
    training_problems:  # ğŸ§ª MINIMAL: Only 3 simple problems for training exclusion
    - gcd
    - get_factors
    - is_valid_parenthesization
  model:
    # cache_dir now comes from paths.{modal|local}.models
    name: codellama/CodeLlama-7b-Python-hf
  name: coral_x_quick_test
  target: quixbugs_codellama

# Two-Loop Architecture: Heavy genes + Cheap knobs
# ğŸ”¥ CRITICAL SEPARATION: Evolution vs Benchmark Parameters
# ========================================================
# EVOLUTION TRAINING: Uses CA-derived cheap knobs (below ranges)
# HELD-OUT BENCHMARK: Uses fixed neutral parameters (see execution.held_out_benchmark_config)
# These are SEPARATE and do NOT interfere with each other!
#
cheap_knobs:
  # ğŸ§¬ EVOLUTION PARAMETERS (CA-derived, per-genome, dynamic) - SAFER RANGES
  temperature_range: [0.3, 0.7]        # ğŸ§ª NARROWER: Safer range for quick test
  top_p_range: [0.8, 0.9]             # ğŸ§ª NARROWER: More focused
  top_k_range: [25, 45]               # ğŸ§ª NARROWER: More focused top-k
  repetition_penalty_range: [1.1, 1.2]  # ğŸ§ª NARROWER: Prevents repetition loops
  max_tokens_range: [100, 200]        # ğŸ§ª SHORTER: Quick generation
  max_new_tokens_range: [100, 200]    # ğŸ§ª SHORTER: Alias for max_tokens_range

infra:
  cache_volume:
    auto_sync: true
    local_path: /Users/kishore/neat/coralx/coral_cache
    modal_mount: /cache
  executor: queue_modal  # ğŸ§® CATEGORY THEORY COMPLIANT - Global queues, natural transformations
  local:
    max_workers: 4
  modal:
    app_name: coral-x-queues  # ğŸ§® Category theory compliant queue-based app
    functions:
      evaluate_genome:
        cpu: 4        # ğŸ§ª REDUCED: Less CPU for test
        gpu: A10G     # ğŸ§ª CHEAPER: A10G instead of A100
        memory: 8192  # ğŸ§ª REDUCED: 8GB instead of 32GB
        timeout: 900  # ğŸ§ª REDUCED: 15min instead of 30min
      generate_code:
        cpu: 2        # ğŸ§ª REDUCED
        gpu: A10G     # ğŸ§ª CHEAPER
        memory: 8192  # ğŸ§ª REDUCED
        timeout: 600  # ğŸ§ª REDUCED
      run_benchmarks:
        cpu: 4        # ğŸ§ª REDUCED
        gpu: A10G     # ğŸ§ª CHEAPER
        memory: 8192  # ğŸ§ª REDUCED
        timeout: 900  # ğŸ§ª REDUCED
      run_experiment:
        cpu: 4        # ğŸ§ª REDUCED
        gpu: A100-40GB # Keep A100 for main experiment
        memory: 16384  # ğŸ§ª REDUCED: 16GB instead of 32GB
        timeout: 1800  # ğŸ§ª REDUCED: 30min instead of 1h
      train_lora:
        cpu: 4        # ğŸ§ª REDUCED
        gpu: A100-40GB # Keep A100 for training
        memory: 16384  # ğŸ§ª REDUCED
        timeout: 900   # ğŸ§ª REDUCED: 15min instead of 30min
    image_config:
      base: debian_slim
      packages:
      - torch
      - transformers
      - accelerate
      - peft
      - datasets
      - numpy
      - scipy
      - scikit-learn
      - pyyaml
      - pytest
      - huggingface_hub
    volume_name: coral-x-clean-cache

seed: 42

system:
  dependencies:
  - numpy
  - torch
  - transformers
  - modal
  - yaml
  fail_fast: true
  version_check: true

threshold:
  base_thresholds:
    bugfix: 0.2     # ğŸ§ª LOWER: Start very low for quick test
    runtime: 0.2
    security: 0.2
    style: 0.2
    syntax: 0.2     # NEW: Start with loose syntax requirements
  max_thresholds:
    bugfix: 0.6     # ğŸ§ª LOWER: End at 0.6 instead of 0.75 for quick test
    runtime: 0.6
    security: 0.6
    style: 0.6
    syntax: 0.7     # NEW: End with moderate syntax requirements
  schedule: linear  # ğŸ§ª SIMPLER: Linear instead of sigmoid

training:
  adam_epsilon: 1e-8
  batch_size: 2             # ğŸ§ª SMALLER: 2 instead of 4
  epochs: 3                 # ğŸ§ª FEWER: 3 instead of 5
  gradient_accumulation_steps: 4  # ğŸ§ª FEWER: 4 instead of 8
  learning_rate: 2e-4
  logging_steps: 25         # ğŸ§ª MORE FREQUENT: 25 instead of 50
  max_grad_norm: 1.0
  save_steps: 100           # ğŸ§ª MORE FREQUENT: 100 instead of 500
  save_strategy: steps
  warmup_steps: 50          # ğŸ§ª FEWER: 50 instead of 100
  weight_decay: 0.01 

 