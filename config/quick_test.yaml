# Quick End-to-End Test Configuration
# Minimal settings for fast system verification

paths:
  modal:
    cache_root: "/cache"
    adapters: "/cache/adapters"
    models: "/cache/models"
    dataset: "/cache/quixbugs_dataset"
    progress: "/cache/evolution_progress.json"
    emergent_behavior: "/cache/emergent_behavior"
    emergent_alerts: "/cache/emergent_behavior/alerts.json"
    realtime_benchmarks: "/cache/realtime_benchmarks"
    coralx_root: "/root/coralx"

cache:
  artifacts_dir: "/cache/adapters"
  base_checkpoint: codellama/CodeLlama-7b-Python-hf
  cleanup_threshold: 100
  metadata: true
  modal_native: true
  run_id: "quick_test_e2e"

evaluation:
  adaptive_testing:
    capability_thresholds:
      easy_cutoff: 0.3
      medium_cutoff: 0.6
    enable: true
    max_easy_problems: 3  # Reduced for speed
    max_hard_problems: 2  # Reduced for speed
    max_medium_problems: 3  # Reduced for speed
  fitness_weights:
    bugfix: 0.3
    runtime: 0.1
    security: 0.25
    style: 0.15
    syntax: 0.2

emergent_tracking:
  enabled: false  # Disabled for quick test

evo:
  alpha_candidates: [4.0, 8.0, 16.0, 32.0]  # More diversity for unique configs
  ca:
    grid_size: [6, 6]  # Slightly larger for more variation
    initial_density: 0.3
    rule_range: [30, 150]  # Wider range for more diversity
    steps_range: [5, 15]   # Wider range
  diversity:
    mode: adaptive  # Use adaptive for better diversity
    base_strength: 1.5  # Higher diversity strength
  dropout_candidates: [0.05, 0.1, 0.15, 0.25]  # More options
  rank_candidates: [4, 8, 16, 32]        # More options
  target_modules: [q_proj, v_proj]  # Keep simple

adapter_type: "lora"  # Use LoRA for speed

execution:
  generations: 2      # Minimal generations
  population_size: 4  # Minimal population
  output_dir: "/cache/results"  # Required field
  selection_mode: "pareto"
  survival_rate: 0.5
  crossover_rate: 0.7
  run_held_out_benchmark: false  # Skip for quick test
  current_generation: 0
  genetic_tracking_dir: "/cache/results/genetic_tracking"

experiment:
  dataset:
    path: "/cache/quixbugs_dataset"
    training_problems:
    - gcd  # Just one simple problem
  model:
    name: codellama/CodeLlama-7b-Python-hf
  name: coral_x_quick_e2e_test
  target: quixbugs_codellama

cheap_knobs:
  temperature_range: [0.3, 0.7]     # Narrow range
  top_p_range: [0.8, 0.9]          # Narrow range
  top_k_range: [30, 50]            # Narrow range
  repetition_penalty_range: [1.1, 1.2]
  max_tokens_range: [150, 250]     # Shorter generation

infra:
  executor: modal
  modal:
    app_name: coral-x-queues

seed: 42

threshold:
  base_thresholds:
    bugfix: 0.2
    runtime: 0.2
    security: 0.3
    style: 0.3
    syntax: 0.2
  max_thresholds:
    bugfix: 0.6
    runtime: 0.6
    security: 0.7
    style: 0.7
    syntax: 0.7
  schedule: sigmoid

training:
  adam_epsilon: 1e-8
  batch_size: 2     # Smaller batch
  epochs: 2         # Fewer epochs
  gradient_accumulation_steps: 4  # Reduced
  learning_rate: 2e-4
  logging_steps: 10
  max_grad_norm: 1.0
  save_steps: 100
  save_strategy: steps
  warmup_steps: 10  # Reduced
  weight_decay: 0.01
