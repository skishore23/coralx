# CoralX Multi-Modal AI Safety with Hyperband Multi-Fidelity Training
# Integrates sophisticated resource-savvy training strategies

# Core CoralX Configuration
coralx_config:
  experiment_name: "multimodal_ai_safety_hyperband"
  description: "Multi-modal AI safety with hyperband multi-fidelity training"
  
  # Evolution parameters
  population_size: 10
  generations: 5
  mutation_rate: 0.3
  crossover_rate: 0.7
  
  # Multi-objective optimization (P1-P6)
  objectives:
    - name: "task_skill"          # P1: Macro-AUROC across tasks
      direction: "maximize"
      weight: 0.3
    - name: "safety"              # P2: 1 - Jailbreak success %
      direction: "maximize"
      weight: 0.25
    - name: "false_positive_cost" # P3: FPR at 90% recall
      direction: "minimize"
      weight: 0.15
    - name: "memory_usage"        # P4: Peak VRAM/RAM during eval
      direction: "minimize"
      weight: 0.1
    - name: "cross_modal_fusion"  # P5: Cross-modal fusion score
      direction: "maximize"
      weight: 0.15
    - name: "calibration"         # P6: -Expected Calibration Error
      direction: "maximize"
      weight: 0.05

# Hyperband Multi-Fidelity Training Configuration
hyperband_config:
  # Enable hyperband instead of simple simulation
  enable_hyperband: true
  
  # Base model configuration
  base_model: "google/gemma-3n-e4b-it"
  model_config:
    torch_dtype: "bfloat16"
    device_map: "auto"
    trust_remote_code: true
  
  # Training stages (Hyperband/Successive Halving)
  training_stages:
    - name: "S0_sanity"
      epoch_budget: 0.05          # 5% of full training
      data_percentage: 5          # 5% of dataset
      survival_rate: 1.0          # All genomes pass (sanity check)
      proxy_metrics: 
        - "gradient_norm"         # Detect gradient explosion
        - "loss_slope"            # Detect non-converging loss
      early_stopping:
        gradient_norm_threshold: 5.0
        loss_slope_threshold: 0.0
        
    - name: "S1_shakeout"
      epoch_budget: 0.3           # 30% of full training
      data_percentage: 20         # 20% of dataset
      survival_rate: 0.5          # Top 50% advance
      proxy_metrics:
        - "text_only_auroc"       # Cheap proxy metric
        - "val_loss"              # Validation loss
      early_stopping:
        min_auroc_threshold: 0.55
        
    - name: "S2_serious"
      epoch_budget: 1.0           # Full single epoch
      data_percentage: 100        # Full dataset
      survival_rate: 0.25         # Top 25% advance
      proxy_metrics:
        - "full_auroc"            # Full multi-modal evaluation
        - "safety_score"          # Safety evaluation
      early_stopping:
        min_auroc_threshold: 0.65
        
    - name: "S3_finisher"
      epoch_budget: 2.0           # Full 2-epoch training
      data_percentage: 100        # Full dataset
      survival_rate: 0.1          # Top 10% complete
      proxy_metrics:
        - "all_metrics"           # Complete P1-P6 evaluation
      early_stopping:
        min_auroc_threshold: 0.75
  
  # Resource sharing configuration
  resource_sharing:
    shared_base_model: true       # Load base model once per node
    shared_dataloaders: true      # Shared memory for data loading
    checkpoint_inheritance: true  # Warm-start from parent checkpoints
    
  # Progressive LoRA configuration
  progressive_lora:
    enable: true
    start_rank: 4                 # Start with low rank
    max_rank: 32                  # Maximum rank for top performers
    rank_growth_schedule: "linear" # How to grow rank over stages
    
  # Cache and checkpointing
  cache_config:
    cache_dir: "cache/hyperband"
    save_checkpoints: true
    checkpoint_frequency: "per_stage"
    cleanup_failed_checkpoints: true

# Dataset Configuration (Multi-Modal)
dataset_config:
  # Fake News Detection
  fake_news:
    enabled: true
    dataset_path: "data/fake_news"
    sample_strategy: "stratified"  # Maintain class balance
    text_max_length: 512
    
  # Deepfake Audio Detection
  deepfake_audio:
    enabled: true
    dataset_path: "data/deepfake_audio"
    sample_strategy: "modal_bucket"  # Balance modalities
    audio_max_duration: 10.0
    short_clip_duration: 3.0      # For early stages
    
  # Deepfake Video Detection
  deepfake_video:
    enabled: true
    dataset_path: "data/deepfake_video"
    sample_strategy: "modal_bucket"
    video_max_duration: 15.0
    frame_sampling_rate: 1.0
    
  # Jailbreak Prompts (Safety)
  jailbreak_prompts:
    enabled: true
    dataset_path: "data/jailbreak_prompts"
    sample_strategy: "uniform"
    prompt_categories: ["harmful", "biased", "private_info"]
    
  # Clean Hold-out Set (False Positive Rate)
  clean_holdout:
    enabled: true
    dataset_path: "data/clean_holdout"
    sample_strategy: "uniform"
    holdout_percentage: 10

# Training Configuration
training_config:
  # Optimizer settings
  optimizer: "adamw"
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 100
  
  # Learning rate scheduling
  scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # Data loading
  batch_size: 4
  dataloader_num_workers: 2
  pin_memory: true
  
  # Mixed precision
  mixed_precision: "bf16"
  gradient_checkpointing: true
  
  # Gradient handling
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4

# Evaluation Configuration
evaluation_config:
  # Metrics computation
  metrics:
    - "auroc"
    - "precision"
    - "recall"
    - "f1_score"
    - "calibration_error"
    - "memory_usage"
    
  # Evaluation frequency
  eval_frequency: "per_stage"
  save_predictions: true
  
  # Safety evaluation
  safety_evaluation:
    jailbreak_test_size: 100
    safety_threshold: 0.8
    
  # Memory monitoring
  memory_monitoring:
    track_peak_memory: true
    track_allocated_memory: true
    memory_profiling: false

# Modal/Distributed Configuration
modal_config:
  # Resource allocation
  gpu_type: "A100-40GB"
  memory_gb: 32
  cpu_count: 8
  timeout_seconds: 3600
  
  # Image configuration
  image_config:
    base_image: "nvcr.io/nvidia/pytorch:23.10-py3"
    python_packages:
      - "transformers>=4.35.0"
      - "peft>=0.6.0"
      - "unsloth>=2023.11"
      - "accelerate>=0.24.0"
      - "bitsandbytes>=0.41.0"
      - "scikit-learn>=1.3.0"
      - "psutil>=5.9.0"
  
  # Queue configuration
  queue_config:
    max_concurrent_jobs: 4
    retry_failed_jobs: true
    job_timeout: 1800

# Logging and Monitoring
logging_config:
  log_level: "INFO"
  log_file: "logs/multimodal_hyperband.log"
  
  # Wandb integration
  wandb:
    enabled: true
    project: "coralx-multimodal-ai-safety"
    tags: ["hyperband", "multimodal", "ai-safety"]
    
  # Progress tracking
  progress_tracking:
    print_frequency: 10
    save_frequency: 100
    checkpoint_frequency: 500

# Hyperband Integration with CoralX Evolution
evolution_integration:
  # How to integrate hyperband results into evolution
  fitness_aggregation: "weighted_sum"  # Combine P1-P6 metrics
  
  # Selection strategy
  selection_strategy: "nsga2"  # Non-dominated sorting
  
  # Mutation with hyperband context
  mutation_config:
    lora_rank_mutation: true
    lora_alpha_mutation: true
    lora_dropout_mutation: true
    ca_seed_mutation: true
    
  # Crossover with checkpoint inheritance
  crossover_config:
    checkpoint_crossover: true    # Inherit from best parent checkpoints
    lora_parameter_crossover: true
    
  # Replacement strategy
  replacement_strategy: "generational"  # Full population replacement
  
  # Elitism with hyperband
  elitism:
    enabled: true
    elite_fraction: 0.1           # Keep top 10% always
    elite_training_budget: "S3_finisher"  # Give elites full training

# Resource Budget Management
budget_management:
  # Total resource budget
  max_training_time_minutes: 120
  max_gpu_hours: 8
  max_memory_gb: 256
  
  # Budget allocation strategy
  budget_allocation:
    early_stages_fraction: 0.3    # 30% budget for S0/S1
    serious_stage_fraction: 0.4   # 40% budget for S2
    finisher_stage_fraction: 0.3  # 30% budget for S3
    
  # Dynamic budget adjustment
  dynamic_budget: true
  min_budget_per_genome: 0.05     # Minimum training budget
  max_budget_per_genome: 2.0      # Maximum training budget 