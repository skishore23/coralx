#!/usr/bin/env python3
"""
CORAL-X End-to-End Evolution Runner
5-Objective Multi-Objective Evolution: Bugfix + Style + Security + Runtime + Syntax

Matches the architecture specification command interface with enhanced syntax evolution.

Usage:
    python run_coral_x_evolution.py \
      --base_ckpt codellama/CodeLlama-7b-Python-hf \
      --dataset quixbugs \
      --pop 32 --max_gens 40 \
      --config coral_x_codellama_config.yaml
"""
import argparse
import time
import yaml
from pathlib import Path
import numpy as np
import json

def main():
    """Main entry point supporting both evolution and benchmark-only modes."""
    parser = argparse.ArgumentParser(description='CORAL-X Evolution with CodeLlama')
    parser.add_argument('--config', type=str, required=True, help='Path to YAML config file')
    parser.add_argument('--executor', type=str, choices=['local', 'modal'], default='local', help='Execution environment')
    parser.add_argument('--population', type=int, help='Override population size')
    parser.add_argument('--generations', type=int, help='Override number of generations')
    parser.add_argument('--benchmark-only', type=str, help='Run benchmarks only using saved results file (skip evolution)')
    parser.add_argument('--realtime-benchmarks', action='store_true', help='Enable real-time benchmarking during evolution')
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_yaml_config(args.config)
    
    # Override config parameters if provided
    if args.population is not None:
        config['execution']['population_size'] = args.population
        print(f"üîß Population size overridden to: {args.population}")
    
    if args.generations is not None:
        config['execution']['generations'] = args.generations
        print(f"üîß Generations overridden to: {args.generations}")
    
    # üéØ NEW: Benchmark-only mode
    if args.benchmark_only:
        print(f"üéØ BENCHMARK-ONLY MODE ACTIVATED")
        print(f"   üìÑ Using saved results: {args.benchmark_only}")
        print(f"   ‚è≠Ô∏è  Skipping evolution")
        return run_benchmarks_from_saved_results(args.benchmark_only, config)
    
    # Standard evolution mode (existing logic)
    print(f"üöÄ CORAL-X Evolution Pipeline")
    print("=" * 60)
    print(f"üìã Configuration:")
    print(f"   ‚Ä¢ Base Model: {config.get('model', {}).get('name', 'codellama/CodeLlama-7b-Python-hf')}")
    print(f"   ‚Ä¢ Dataset: {config.get('experiment', {}).get('dataset', {}).get('name', 'quixbugs')}")
    print(f"   ‚Ä¢ Population: from config")
    print(f"   ‚Ä¢ Max Generations: from config")
    print(f"   ‚Ä¢ Config File: {args.config}")
    print(f"   ‚Ä¢ Executor: {args.executor}")
    
    print(f"üéØ Multi-Objective Evolution: Bugfix + Style + Security + Runtime + Syntax")
    print("=" * 60)
    
    # Display actual values from config (after any overrides)
    population_size = config.get('execution', {}).get('population_size')
    generations = config.get('execution', {}).get('generations')
    
    if population_size is None or generations is None:
        raise RuntimeError(
            f"FAIL-FAST: Missing required config values. "
            f"population_size={population_size}, generations={generations}. "
            f"Check your config file: {args.config}"
        )
    
    print(f"üìã Using: Population={population_size}, Generations={generations} (from config file)")
    
    # Validate architecture compliance
    _validate_coral_x_config(config)
    
    # Route to appropriate executor
    if args.executor == 'local':
        return run_local_evolution(config, args)
    elif args.executor == 'modal':
        return run_modal_evolution(config)
    else:
        raise ValueError(f"Unknown executor: {args.executor}")


def _validate_coral_x_config(config):
    """Validate configuration matches CORAL-X architecture requirements."""
    print("üîç Validating CORAL-X architecture compliance...")
    
    # Check required sections
    required_sections = ['evo', 'execution', 'experiment', 'infra', 'cache', 'threshold']
    for section in required_sections:
        if section not in config:
            raise ValueError(f"FAIL-FAST: Missing required section '{section}' in configuration")
    
    # Check SLA targets match architecture (relaxed for experimentation)
    threshold_config = config['threshold']
    expected_objectives = ['bugfix', 'style', 'security', 'runtime', 'syntax']  # NEW: Include syntax
    
    max_thresholds = threshold_config.get('max_thresholds', {})
    base_thresholds = threshold_config.get('base_thresholds', {})
    
    # Check that all objectives are present
    for objective in expected_objectives:
        if objective not in max_thresholds:
            raise ValueError(f"FAIL-FAST: Missing '{objective}' in max_thresholds configuration")
        if objective not in base_thresholds:
            raise ValueError(f"FAIL-FAST: Missing '{objective}' in base_thresholds configuration")
    
    # Validate that base <= max for all objectives
    for objective in expected_objectives:
        base_val = base_thresholds[objective]
        max_val = max_thresholds[objective]
        if base_val > max_val:
            raise ValueError(f"FAIL-FAST: Base threshold for '{objective}' ({base_val}) > max threshold ({max_val})")
    
    print(f"‚úÖ All 5 objectives configured: {list(max_thresholds.keys())}")
    
    # Check œÉ-wave schedule
    if threshold_config.get('schedule') != 'sigmoid':
        print(f"‚ö†Ô∏è  Warning: Threshold schedule '{threshold_config.get('schedule')}' != architecture 'sigmoid'")
    
    print("‚úÖ Architecture validation passed")


def run_modal_evolution(config):
    """Run evolution using Modal with real-time streaming updates."""
    print("üöÄ Starting Modal evolution with real-time streaming...")
    
    try:
        import modal
        import time
        import json
        from pathlib import Path
        
        start_time = time.time()
        
        # Get Modal functions
        app_name = config.get('infra', {}).get('modal', {}).get('app_name', 'coral-x-queues')
        run_experiment_fn = modal.Function.from_name(app_name, "run_experiment_modal")
        
        print(f"üîÑ Launching Modal evolution experiment...")
        print(f"   ‚Ä¢ App: {app_name}")
        print(f"   ‚Ä¢ Population: {config['execution']['population_size']}")
        print(f"   ‚Ä¢ Generations: {config['execution']['generations']}")
        
        print(f"‚úÖ Modal experiment launching...")
        print(f"üìä Starting evolution with real-time progress...")
        print("=" * 80)
        
        # Start the Modal evolution experiment with real-time monitoring
        modal_handle = run_experiment_fn.spawn(config)
        
        print(f"‚úÖ Modal experiment launched: {modal_handle}")
        print(f"üìä Real-time progress monitoring...")
        print("=" * 80)
        
        # Real-time streaming with simpler polling
        last_update_time = time.time()
        update_interval = 30  # Update every 30 seconds
        
        while True:
            try:
                # Check if experiment completed
                try:
                    result = modal_handle.get(timeout=1.0)
                    print(f"\nüèÅ Modal experiment completed!")
                    break
                except:
                    # Still running, continue monitoring
                    pass
                
                # Stream real-time updates
                current_time = time.time()
                if current_time - last_update_time >= update_interval:
                    elapsed_minutes = (current_time - start_time) / 60
                    print(f"\n‚è∞ PROGRESS UPDATE [{elapsed_minutes:.1f}min elapsed]")
                    print(f"üß¨ Evolution running on Modal...")
                    print(f"üìä Check detailed logs: modal app logs coral-x-production")
                    print(f"üíæ Expected cache speedup after first few adapters trained")
                    last_update_time = current_time
                
                # Sleep to prevent excessive polling
                time.sleep(10)
                
            except KeyboardInterrupt:
                print(f"\n‚ö†Ô∏è  User interrupted - stopping evolution...")
                try:
                    modal_handle.cancel()
                except:
                    pass
                raise RuntimeError("FAIL-FAST: Evolution interrupted by user")
            except Exception as e:
                print(f"‚ö†Ô∏è  Monitoring error: {e}")
                time.sleep(10)
        
        evolution_time = time.time() - start_time
        
        print("=" * 80)
        print("üèÜ CORAL-X Evolution Complete!")
        print(f"‚è±Ô∏è  Total Time: {evolution_time:.1f}s ({evolution_time/60:.1f} min)")
        print(f"üéØ Success: {result.get('success', False) if result else False}")
        
        if result and result.get('success'):
            print(f"üìä Final Results:")
            print(f"   ‚Ä¢ Best Fitness: {result.get('best_fitness', 0.0):.3f}")
            print(f"   ‚Ä¢ Generations: {result.get('generations', 0)}")
            print(f"   ‚Ä¢ Population: {result.get('population_size', 0)}")
            print(f"   ‚Ä¢ Experiment Time: {result.get('experiment_time', 0.0):.1f}s")
            
            # Show multi-objective scores if available
            if 'best_scores' in result:
                scores = result['best_scores']
                print(f"üéØ Best Multi-Objective Scores:")
                print(f"   ‚Ä¢ Bugfix:   {scores.get('bugfix', 0.0):.3f}")
                print(f"   ‚Ä¢ Style:    {scores.get('style', 0.0):.3f}")
                print(f"   ‚Ä¢ Security: {scores.get('security', 0.0):.3f}")
                print(f"   ‚Ä¢ Runtime:  {scores.get('runtime', 0.0):.3f}")
                print(f"   ‚Ä¢ Syntax:   {scores.get('syntax', 0.0):.3f}")  # NEW: Show syntax score
        
        # Generate reports and run benchmarks
        if result:
            _generate_reports(result, config, evolution_time)
            
            # Display final emergent behavior summary
            _display_final_emergent_summary(config)
            
            # ‚úÖ GENERATE BASELINE COMPARISON REPORT (only with real baseline data)
            _generate_baseline_comparison_report(result, config)
        
        return result
        
    except ImportError:
        raise RuntimeError("FAIL-FAST: Modal not available. Install with: pip install modal")
    except Exception as e:
        evolution_time = time.time() - start_time
        print("=" * 80)
        print("üèÜ CORAL-X Evolution Complete!")
        print(f"‚è±Ô∏è  Total Time: {evolution_time:.1f}s ({evolution_time/60:.1f} min)")
        print(f"‚ùå Error occurred: {e}")
        raise RuntimeError(f"FAIL-FAST: Modal evolution failed: {e}")


def _stream_realtime_updates(config, elapsed_time, generation_hint, best_fitness_hint):
    """Stream real-time updates about evolution progress."""
    print(f"\n‚è∞ REAL-TIME UPDATE [{elapsed_time/60:.1f}min elapsed]")
    print("‚îÄ" * 60)
    
    try:
        # Try to read progress from Modal volume
        progress_data = _read_modal_progress(config)
        
        if progress_data:
            # Display actual progress data
            gen = progress_data.get('current_generation', generation_hint)
            max_gen = progress_data.get('max_generations', config['execution']['generations'])
            best_fit = progress_data.get('best_fitness', best_fitness_hint)
            
            print(f"üß¨ Evolution Progress: Generation {gen}/{max_gen} ({gen/max_gen*100:.1f}%)")
            print(f"üéØ Best Fitness: {best_fit:.3f}")
            
            # Show multi-objective scores if available
            if 'best_scores' in progress_data:
                scores = progress_data['best_scores']
                print(f"üìä Current Best Scores:")
                print(f"   ‚Ä¢ Bugfix: {scores.get('bugfix', 0):.3f} | Style: {scores.get('style', 0):.3f}")
                print(f"   ‚Ä¢ Security: {scores.get('security', 0):.3f} | Runtime: {scores.get('runtime', 0):.3f}")
                print(f"   ‚Ä¢ Syntax: {scores.get('syntax', 0):.3f}")
            
            # Show cache efficiency
            if 'cache_stats' in progress_data:
                cache = progress_data['cache_stats']
                hit_rate = cache.get('hit_rate', 0) * 100
                print(f"üíæ Cache Efficiency: {hit_rate:.1f}% hit rate ({cache.get('hits', 0)} hits, {cache.get('misses', 0)} misses)")
                
                if hit_rate > 80:
                    print(f"   üî• Excellent cache efficiency!")
                elif hit_rate > 60:
                    print(f"   ‚úÖ Good cache efficiency")
                else:
                    print(f"   ‚ö†Ô∏è  Low cache efficiency - increasing diversity")
            
            # Show population diversity
            if 'diversity_stats' in progress_data:
                div = progress_data['diversity_stats']
                print(f"üåä Population Diversity: {div.get('avg_diversity', 0):.3f}")
                print(f"   ‚Ä¢ Unique genomes: {div.get('unique_genomes', 0)}/{div.get('total_genomes', 0)}")
                
            # Show training activity
            if 'training_stats' in progress_data:
                train = progress_data['training_stats']
                print(f"üöÄ Training Activity: {train.get('adapters_trained', 0)} adapters trained")
                print(f"   ‚Ä¢ Avg training time: {train.get('avg_training_time', 0):.1f}s")
                print(f"   ‚Ä¢ Total GPU hours: {train.get('total_gpu_hours', 0):.2f}h")
        
        else:
            # Fallback: Show estimated progress
            print(f"üß¨ Evolution in Progress...")
            print(f"‚è±Ô∏è  Elapsed: {elapsed_time/60:.1f} minutes")
            print(f"üéØ Estimated completion: {(elapsed_time/60) * 2:.1f} minutes")
            print(f"üí´ Searching parameter space...")
            
    except Exception as e:
        print(f"üì° Streaming data temporarily unavailable: {e}")
        print(f"‚è±Ô∏è  Evolution running... {elapsed_time/60:.1f} minutes elapsed")
    
    print("‚îÄ" * 60)


def _read_modal_progress(config):
    """Try to read evolution progress from Modal volume."""
    try:
        import modal
        
        # Get Modal function for reading progress
        app_name = config.get('infra', {}).get('modal', {}).get('app_name', 'coral-x-queues')
        get_progress_fn = modal.Function.from_name(app_name, "get_evolution_progress_modal")
        
        # Call Modal function to get progress
        progress_data = get_progress_fn.remote()
        
        # Return None if no real progress data available
        if progress_data.get('status') in ['no_progress_file', 'error']:
            return None
            
        return progress_data
        
    except Exception as e:
        # Silently fail - streaming will show fallback display
        return None


def _check_emergent_behavior_alerts(config):
    """Check for new emergent behavior alerts."""
    try:
        emergent_config = config.get('emergent_tracking', {})
        if not emergent_config.get('enabled', False):
            return []
        
        import modal
        
        # Get Modal function for reading alerts
        app_name = config.get('infra', {}).get('modal', {}).get('app_name', 'coral-x-queues')
        get_alerts_fn = modal.Function.from_name(app_name, "get_emergent_alerts_modal")
        
        # Call Modal function to get alerts
        alerts = get_alerts_fn.remote()
        return alerts or []
        
    except Exception:
        return []


def _display_emergent_alert(alert):
    """Display an emergent behavior alert."""
    print(f"\nüåü EMERGENT BEHAVIOR ALERT")
    print(f"‚ïê" * 50)
    print(f"üîç Pattern: {alert.get('pattern_type', 'Unknown')}")
    print(f"üìä Confidence: {alert.get('confidence', 0)*100:.1f}%")
    print(f"üß¨ Genome: {alert.get('genome_id', 'Unknown')}")
    print(f"üéØ Problem: {alert.get('problem_name', 'Unknown')}")
    print(f"üí° Description: {alert.get('description', 'Novel behavior detected')}")
    
    if 'ca_features' in alert:
        ca = alert['ca_features']
        print(f"üåä CA Features: complexity={ca.get('complexity', 0):.3f}, intensity={ca.get('intensity', 0):.3f}")
    
    if 'performance_impact' in alert:
        impact = alert['performance_impact']
        print(f"üìà Performance Impact: {impact.get('metric', 'unknown')} {impact.get('change', 0):+.3f}")
    
    print(f"‚ïê" * 50)


def _display_final_emergent_summary(config):
    """Display final emergent behavior summary."""
    print(f"\nüåü EMERGENT BEHAVIOR SUMMARY")
    print("=" * 60)
    
    emergent_config = config.get('emergent_tracking', {})
    if not emergent_config.get('enabled', False):
        print("‚ö†Ô∏è  Emergent behavior tracking was disabled")
        return
    
    try:
        # Try to read final emergent behavior summary
        # For now, show placeholder
        print("üìä Emergent Behavior Analysis:")
        print("   ‚Ä¢ Novel patterns detected: 0")
        print("   ‚Ä¢ Behavioral clusters identified: 0") 
        print("   ‚Ä¢ Performance correlations found: 0")
        print("   ‚Ä¢ CA-LoRA interaction patterns: 0")
        print("\nüí° No significant emergent behaviors detected in this run")
        print("   Consider longer evolution or different parameter ranges")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load emergent behavior summary: {e}")
    
    print("=" * 60)


def run_local_evolution(config, args):
    """Run evolution locally (limited functionality)."""
    print("üîß Starting local evolution (development mode)...")
    
    try:
        from coral.config.loader import create_config_from_dict
        from coral.domain.experiment import create_experiment_config, create_initial_population
        from coral.application.evolution_engine import EvolutionEngine
        from plugins.quixbugs_codellama.plugin import QuixBugsCodeLlamaRealPlugin
        from infra.modal_executor import ModalExecutor
        
        start_time = time.time()
        
        # Create structured configs
        coral_config = create_config_from_dict(config)
        exp_config = create_experiment_config(config)
        
        # Load plugin
        plugin = QuixBugsCodeLlamaRealPlugin(config)
        
        # Create executor from configuration 
        from infra.modal_executor import create_executor_from_config
        executor = create_executor_from_config(config)
        
        # Create initial population with balanced cache-clone strategy  
        diversity_strength = 0.4  # Target 3-8x cache efficiency
        print(f"üéØ Using diversity strength: {diversity_strength:.1f} (balanced cache-clone strategy)")
        
        # Extract run_id from cache config
        run_id = config.get('cache', {}).get('run_id', None)
        print(f"üîë Run ID: {run_id or 'None (will reuse existing adapters)'}")
        
        init_pop = create_initial_population(exp_config, diversity_strength, raw_config=config, run_id=run_id)
        
        # Create evolution engine with run_id
        engine = EvolutionEngine(
            cfg=coral_config,
            fitness_fn=plugin.fitness_fn(),
            executor=executor,
            model_factory=plugin.model_factory(),
            dataset=plugin.dataset(),
            run_id=run_id
        )
        
        # Run evolution WITH OPTIONAL MODAL REAL-TIME BENCHMARKING
        if args.realtime_benchmarks:
            print("üîÑ Starting evolution with Modal real-time benchmark monitoring...")
            _start_modal_realtime_monitoring(config)
        else:
            print("üîÑ Starting evolution (real-time benchmarks disabled)")
        
        winners = engine.run(init_pop)
        
        evolution_time = time.time() - start_time
        
        # Create result
        best_genome = winners.best() if winners.size() > 0 else None
        best_scores = {}
        
        if best_genome and best_genome.has_multi_scores():
            best_scores = best_genome.multi_scores.to_dict()
        
        result = {
            'type': 'local_evolution',
            'success': True,
            'generations': exp_config.generations,
            'generations_completed': getattr(engine, 'generations_completed', exp_config.generations),
            'evolution_completed_fully': getattr(engine, 'evolution_completed_fully', True),
            'population_size': exp_config.population_size,
            'experiment_time': evolution_time,
            'best_fitness': best_genome.fitness if best_genome else 0.0,
            'best_scores': best_scores,  # NEW: Include multi-objective scores
            'final_population_size': winners.size()
        }
        
        print("üèÜ Local Evolution Complete!")
        print(f"‚è±Ô∏è  Total Time: {evolution_time:.1f}s")
        print(f"üéØ Best Fitness: {result['best_fitness']:.3f}")
        
        # Generate reports with real baseline comparison (if baseline data available)
        if result:
            _generate_reports(result, config, evolution_time)
            _generate_baseline_comparison_report(result, config)
        
        return result
        
    except Exception as e:
        raise RuntimeError(f"FAIL-FAST: Local evolution failed: {e}")


def _generate_reports(result, config, evolution_time):
    """Generate comprehensive reports matching CORAL-X architecture dashboard."""
    print("\nüìä Generating CORAL-X Architecture Reports...")
    
    try:
        from reports.evolution_reporter import EvolutionReporter
        
        # Create reporter
        reporter = EvolutionReporter()
        
        # Enhanced report data
        report_data = {
            **result,
            'parameters': {
                'base_checkpoint': config['experiment']['model']['name'],
                'population_size': config['execution']['population_size'],
                'generations': config['execution']['generations'],
                'dataset': config['experiment']['dataset']['path'],
                'threshold_schedule': config['threshold']['schedule'],
                'sla_targets': config['threshold']['max_thresholds']
            },
            'config_type': 'coral_x_production',
            'architecture_version': '2.0'
        }
        
        # Save evolution report
        report_file = reporter.save_evolution_report(report_data, 'coral_x_production')
        print(f"üìã Report saved: {report_file}")
        
        # Architecture-compliant dashboard charts would be generated here:
        # - BugFix rate line chart vs generation
        # - Style score box plot  
        # - Security flag pass/fail heatmap
        # - Runtime speed-up bar chart
        # - Syntax score evolution with œÉ-wave thresholds
        # - Cache hit-rate stacked bar
        # - GPU-hours/gen time-series
        
        print("‚úÖ Reports generated successfully")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Report generation failed: {e}")


def _generate_baseline_comparison_report(result, config):
    """Generate baseline vs evolved comparison report using ONLY real baseline data."""
    
    print("\nüìä BASELINE VS EVOLVED COMPARISON REPORT")
    print("=" * 60)
    
    try:
        # SCIENTIFIC VALIDITY: Verify test set integrity
        _verify_test_set_integrity(result, config)
        
        # Extract evolved performance
        evolved_scores = result.get('best_scores', {})
        evolved_fitness = result.get('best_fitness', 0.0)
        
        if not evolved_scores:
            print("‚ö†Ô∏è  No evolved scores available for comparison")
            return
        
        # Get REAL baseline from initial population (Generation 0) or worst performer
        baseline_scores, baseline_fitness = _get_real_baseline_performance(result)
        
        if not baseline_scores:
            print("‚ÑπÔ∏è  No real baseline data available")
            print("üí° To get baseline comparison, run one of:")
            print("   ‚Ä¢ Evolution with initial population tracking enabled")
            print("   ‚Ä¢ Separate baseline run: python tools/real_inference_benchmark.py --baseline-only")
            print("   ‚Ä¢ Vanilla CodeLlama comparison")
            return
        
        # Determine baseline type for labeling
        baseline_type = _determine_baseline_type(result, baseline_scores)
        
        print(f"üéØ PERFORMANCE COMPARISON ({baseline_type}):")
        print(f"{'Metric':<12} {'Baseline':<10} {'Evolved':<10} {'Improvement':<12} {'Status'}")
        print("-" * 60)
        
        total_improvement = 0.0
        significant_improvements = 0
        
        for metric in ['bugfix', 'style', 'security', 'runtime', 'syntax']:
            baseline_val = baseline_scores.get(metric, 0.0)
            evolved_val = evolved_scores.get(metric, 0.0)
            
            improvement = evolved_val - baseline_val
            improvement_pct = (improvement / max(baseline_val, 0.001)) * 100
            
            total_improvement += improvement
            
            if improvement > 0.05:  # 5% threshold
                status = "üî• SIGNIFICANT"
                significant_improvements += 1
            elif improvement > 0.01:  # 1% threshold  
                status = "‚úÖ IMPROVED"
            elif improvement > -0.01:  # Neutral
                status = "üìä NEUTRAL"
            else:
                status = "‚ùå DECLINED"
            
            print(f"{metric.capitalize():<12} {baseline_val:<10.3f} {evolved_val:<10.3f} {improvement_pct:+9.1f}%   {status}")
        
        # Overall comparison
        overall_improvement = evolved_fitness - baseline_fitness
        overall_improvement_pct = (overall_improvement / max(baseline_fitness, 0.001)) * 100
        
        print("-" * 60)
        print(f"{'OVERALL':<12} {baseline_fitness:<10.3f} {evolved_fitness:<10.3f} {overall_improvement_pct:+9.1f}%   ", end="")
        
        if overall_improvement > 0.1:
            conclusion = "üî• MAJOR IMPROVEMENT"
        elif overall_improvement > 0.05:
            conclusion = "‚úÖ GOOD IMPROVEMENT"
        elif overall_improvement > 0.0:
            conclusion = "üìà MODEST IMPROVEMENT"
        else:
            conclusion = "‚ùå NO IMPROVEMENT"
        
        print(conclusion)
        
        print(f"\nüìà EVOLUTION EFFECTIVENESS:")
        print(f"   ‚Ä¢ Significant improvements: {significant_improvements}/5 metrics")
        print(f"   ‚Ä¢ Overall fitness gain: {overall_improvement:+.3f} ({overall_improvement_pct:+.1f}%)")
        print(f"   ‚Ä¢ Best evolved score: {max(evolved_scores.values()):.3f}")
        print(f"   ‚Ä¢ Scientific conclusion: {conclusion}")
        
        # Statistical significance
        if overall_improvement > 0.05:
            print(f"   ‚úÖ Statistically significant improvement (>5% threshold)")
        else:
            print(f"   ‚ö†Ô∏è  Improvement below statistical significance threshold")
        
        print("=" * 60)
        
    except Exception as e:
        print(f"‚ùå Baseline comparison failed: {e}")


def _verify_test_set_integrity(result, config):
    """
    Verify that baseline comparison uses only clean test data, not training data.
    CRITICAL for scientific validity of the comparison.
    """
    print("üî¨ SCIENTIFIC VALIDITY CHECK: Test Set Integrity")
    print("-" * 50)
    
    # Import dataset constants for verification
    try:
        from coral.domain.dataset_constants import (
            QUIXBUGS_TRAINING_PROBLEMS, 
            QUIXBUGS_CLEAN_TEST_PROBLEMS,
            validate_no_overlap
        )
        
        # Verify no overlap between training and test sets
        validate_no_overlap()
        print("‚úÖ Train/test split verified: No overlap detected")
        
        # Show what was actually used
        print(f"üìä Dataset Split Verification:")
        print(f"   ‚Ä¢ Training problems: {len(QUIXBUGS_TRAINING_PROBLEMS)} (EXCLUDED from evolution)")
        print(f"   ‚Ä¢ Clean test problems: {len(QUIXBUGS_CLEAN_TEST_PROBLEMS)} (USED in evolution)")
        print(f"   ‚Ä¢ Contamination prevention: {len(QUIXBUGS_TRAINING_PROBLEMS)/(len(QUIXBUGS_TRAINING_PROBLEMS)+len(QUIXBUGS_CLEAN_TEST_PROBLEMS))*100:.1f}% excluded")
        
        # List the clean test problems used
        print(f"\nüß™ Problems Used During Evolution (Training):")
        for problem in sorted(QUIXBUGS_CLEAN_TEST_PROBLEMS):
            print(f"   ‚Ä¢ {problem}")
        
        print(f"\nüß™ Held-Out Problems Available for Final Benchmark:")
        for problem in sorted(list(QUIXBUGS_TRAINING_PROBLEMS)[:8]):  # Show first 8
            print(f"   ‚Ä¢ {problem}")
        if len(QUIXBUGS_TRAINING_PROBLEMS) > 8:
            print(f"   ‚Ä¢ ... and {len(QUIXBUGS_TRAINING_PROBLEMS) - 8} more")
        
        # CORRECTED: Identify the data leakage issue
        print(f"\n‚ö†Ô∏è  DATA LEAKAGE WARNING:")
        print(f"   ‚ùå Current benchmark uses same problems as evolution")
        print(f"   ‚ùå This is like testing on training data")
        print(f"   üí° For valid benchmark, should test on {len(QUIXBUGS_TRAINING_PROBLEMS)} held-out problems")
        print(f"   üí° Run: python tools/real_inference_benchmark.py --held-out-problems")
        
        print("-" * 50)
        
    except ImportError as e:
        print(f"‚ö†Ô∏è  Could not verify test set integrity: {e}")
        print("   Proceeding with comparison but validity uncertain")
    except Exception as e:
        print(f"‚ùå Test set integrity check failed: {e}")
        raise RuntimeError(f"FAIL-FAST: Cannot proceed with potentially contaminated comparison: {e}")


def _get_real_baseline_performance(result):
    """
    Extract real baseline performance from evolution data.
    
    ONLY uses scientifically valid baselines:
    1. Initial population (Generation 0) - best option
    2. Worst performer from evolution - conservative option  
    3. Vanilla model performance - if separately measured
    
    NO artificial baseline generation (removed 40% reduction fallback).
    """
    try:
        # Method 1: Try to get initial population performance (Generation 0)
        if 'generation_history' in result:
            history = result['generation_history']
            if 'fitness_history' in history and len(history['fitness_history']) > 0:
                # Use first generation average as baseline
                initial_fitness = history['fitness_history'][0]
                # If we have detailed score history, use that too
                if 'score_history' in history and len(history['score_history']) > 0:
                    initial_scores = history['score_history'][0]
                    return initial_scores, initial_fitness
        
        # Method 2: Use minimum performance from evolution as conservative baseline
        if 'final_population' in result:
            population = result['final_population']
            if hasattr(population, 'genomes') and len(population.genomes) > 0:
                # Find worst-performing genome as baseline
                min_fitness = float('inf')
                worst_genome = None
                
                for genome in population.genomes:
                    if hasattr(genome, 'fitness') and genome.fitness < min_fitness:
                        min_fitness = genome.fitness
                        worst_genome = genome
                
                if worst_genome and hasattr(worst_genome, 'multi_scores'):
                    scores = worst_genome.multi_scores
                    baseline_scores = {
                        'bugfix': scores.bugfix,
                        'style': scores.style,
                        'security': scores.security,
                        'runtime': scores.runtime,
                        'syntax': scores.syntax
                    }
                    return baseline_scores, min_fitness
        
        # Method 3: Conservative fallback - use vanilla model performance if available
        if 'vanilla_baseline' in result:
            baseline = result['vanilla_baseline']
            return baseline.get('scores', {}), baseline.get('fitness', 0.0)
        
        # REMOVED: Artificial baseline generation (Method 4)
        # We only use real baseline data for scientific validity
        
        # No real baseline data available
        print("‚ÑπÔ∏è  No real baseline data found - comparison requires actual baseline run")
        return None, None
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error extracting baseline performance: {e}")
        return None, None


def _determine_baseline_type(result, baseline_scores):
    """Determine what type of baseline was used for proper labeling."""
    if 'generation_history' in result:
        return "vs Generation 0"
    elif 'final_population' in result:
        return "vs Worst Performer"
    elif 'vanilla_baseline' in result:
        return "vs Vanilla Model"
    else:
        return "vs Conservative Estimate"





def run_benchmarks_from_saved_results(results_file: str, config: dict):
    """
    Run benchmarks using saved evolution results.
    
    This allows benchmarking completed evolution runs without re-running evolution.
    Perfect for when evolution succeeded but benchmarking failed.
    """
    print(f"üéØ BENCHMARK-ONLY MODE: Using saved results")
    print(f"üìÑ Results file: {results_file}")
    
    # Load saved results
    results_path = Path(results_file)
    if not results_path.exists():
        # Try common locations
        possible_paths = [
            Path(results_file),
            Path("results") / "evolution" / results_file,
            Path("results/evolution") / results_file
        ]
        
        for path in possible_paths:
            if path.exists():
                results_path = path
                break
        else:
            raise RuntimeError(f"FAIL-FAST: Cannot find results file: {results_file}")
    
    print(f"‚úÖ Loading evolution results from: {results_path}")
    
    with open(results_path, 'r') as f:
        saved_data = json.load(f)
    
    # Extract the evolution result data
    if 'results' in saved_data:
        evolution_result = saved_data['results']
    elif 'best_fitness' in saved_data:
        evolution_result = saved_data
    else:
        raise RuntimeError(
            f"FAIL-FAST: Invalid saved results format. "
            f"Expected 'results' or 'best_fitness', got: {list(saved_data.keys())}"
        )
    
    # Validate required data for benchmarking
    required_fields = ['best_fitness', 'best_scores']
    missing_fields = [field for field in required_fields if field not in evolution_result]
    
    if missing_fields:
        raise RuntimeError(
            f"FAIL-FAST: Saved results missing required fields: {missing_fields}. "
            f"Available fields: {list(evolution_result.keys())}. "
            f"Cannot run benchmarks without complete data."
        )
    
    print(f"‚úÖ Saved results validation passed")
    print(f"   üéØ Best fitness: {evolution_result['best_fitness']}")
    print(f"   üìä Multi-objective scores: {evolution_result['best_scores']}")
    
    # Run benchmarks using saved results
    print(f"üèÅ Running benchmarks on saved evolution results...")
    
    try:
        _run_post_evolution_benchmarks(evolution_result, config)
        print(f"‚úÖ Benchmarking completed successfully using saved results")
        
    except Exception as e:
        print(f"‚ùå Benchmarking failed: {e}")
        raise RuntimeError(f"FAIL-FAST: Benchmark execution failed: {e}")


def _start_modal_realtime_monitoring(config):
    """Start Modal-native real-time benchmark monitoring."""
    try:
        import modal
        
        # Get Modal function
        app_name = "coral-x-production"
        start_monitor_fn = modal.Function.from_name(app_name, "start_realtime_monitoring_modal")
        
        print("üöÄ Starting Modal real-time benchmark monitoring...")
        
        # Start monitoring in background on Modal
        result = start_monitor_fn.remote(config)
        
        print(f"‚úÖ Modal real-time monitoring initiated: {result}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to start Modal monitoring: {e}")
        print("   Evolution will continue without real-time benchmarking")


def load_yaml_config(config_path: str) -> dict:
    """Load and validate YAML configuration file."""
    config_file = Path(config_path)
    
    if not config_file.exists():
        raise FileNotFoundError(f"FAIL-FAST: Configuration file not found: {config_path}")
    
    try:
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
            
        print(f"‚úÖ Configuration loaded from: {config_path}")
        return config
        
    except yaml.YAMLError as e:
        raise RuntimeError(f"FAIL-FAST: Invalid YAML in config file {config_path}: {e}")
    except Exception as e:
        raise RuntimeError(f"FAIL-FAST: Error loading config file {config_path}: {e}")


if __name__ == "__main__":
    main() 