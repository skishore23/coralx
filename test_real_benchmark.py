#!/usr/bin/env python3
"""
Test Real Inference Benchmark
==============================

Quick test to verify the benchmark infrastructure works.
"""
import sys
sys.path.append('.')

def test_dependencies():
    """Test if we have required dependencies for real inference."""
    
    print("üîß Testing Real Inference Dependencies")
    print("=" * 50)
    
    try:
        import torch
        print(f"‚úÖ PyTorch: {torch.__version__}")
        print(f"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}")
    except ImportError:
        print("‚ùå PyTorch not available")
    
    try:
        import transformers
        print(f"‚úÖ Transformers: {transformers.__version__}")
    except ImportError:
        print("‚ùå Transformers not available")
    
    try:
        import peft
        print(f"‚úÖ PEFT: {peft.__version__}")
    except ImportError:
        print("‚ùå PEFT not available")
    
    try:
        from coral.domain.dataset_constants import QUIXBUGS_CLEAN_TEST_PROBLEMS
        print(f"‚úÖ Clean test problems: {len(QUIXBUGS_CLEAN_TEST_PROBLEMS)}")
    except ImportError as e:
        print(f"‚ùå Dataset constants failed: {e}")
    
    try:
        from real_inference_benchmark import RealInferenceBenchmark
        print(f"‚úÖ Real inference benchmark imported")
    except ImportError as e:
        print(f"‚ùå Benchmark import failed: {e}")


def test_benchmark_initialization():
    """Test benchmark initialization."""
    
    print("\nüöÄ Testing Benchmark Initialization")
    print("=" * 50)
    
    try:
        from real_inference_benchmark import RealInferenceBenchmark
        
        # Test with clean config
        benchmark = RealInferenceBenchmark("coral_x_clean_config.yaml")
        print(f"‚úÖ Benchmark initialized successfully")
        print(f"   ‚Ä¢ Base model: {benchmark.base_model}")
        print(f"   ‚Ä¢ Clean problems loaded: {len(benchmark.clean_problems)}")
        
        # Show first few problems
        if benchmark.clean_problems:
            print(f"   ‚Ä¢ First problem: {benchmark.clean_problems[0].get('name', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Benchmark initialization failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dry_run():
    """Test a dry run without actual inference."""
    
    print("\nüéØ Testing Dry Run")
    print("=" * 50)
    
    try:
        from real_inference_benchmark import RealInferenceBenchmark
        
        benchmark = RealInferenceBenchmark("coral_x_clean_config.yaml")
        
        if not benchmark.clean_problems:
            print("‚ùå No clean problems available for testing")
            return False
        
        # Test problem loading
        test_problem = benchmark.clean_problems[0]
        print(f"‚úÖ Test problem: {test_problem.get('name')}")
        
        # Test evaluation framework (without actual inference)
        print(f"‚úÖ Evaluation framework accessible")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Dry run failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run all tests."""
    
    print("üîß REAL INFERENCE BENCHMARK VALIDATION")
    print("=" * 60)
    
    # Test dependencies
    test_dependencies()
    
    # Test initialization
    init_success = test_benchmark_initialization()
    
    # Test dry run
    if init_success:
        dry_run_success = test_dry_run()
        
        if dry_run_success:
            print("\nüéâ ALL TESTS PASSED!")
            print("‚úÖ Ready to run real inference benchmark")
            print("\nTo run the benchmark:")
            print("   python real_inference_benchmark.py")
        else:
            print("\n‚ö†Ô∏è Dry run failed - check configuration")
    else:
        print("\n‚ùå Initialization failed - check dependencies")


if __name__ == "__main__":
    main() 